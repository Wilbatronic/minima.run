import Foundation
import Metal
import CoreML
import Vision

@available(macOS 14.0, iOS 17.0, *)
public class VisionProcessor {
    
    // Dependencies
    private let textureUtils = TextureUtils.shared
    private let device = MTLCreateSystemDefaultDevice()!
    
    // CoreML Model (Placeholder for the generated class)
    // In production, this would be `try? QwenVisionEncoder(configuration: config)`
    private var visionModel: VNCoreMLModel?
    
    // Buffers for intermediate state
    // We reuse these to avoid allocation every frame
    private var coarseTexture: MTLTexture?
    private var normalizedBuffer: MTLBuffer?
    
    // Callback to the Brain (llama.cpp)
    public var onEmbeddingsGenerated: ((MLMultiArray) -> Void)?
    
    public init() {
        setupCoreML()
    }
    
    private func setupCoreML() {
        // Load the compiled CoreML model
        // Assuming 'QwenVisionEncoder' is valid class generated by Xcode from .mlpackage
        /*
        do {
            let config = MLModelConfiguration()
            config.computeUnits = .all // Uses ANE + GPU + CPU hybrid
            let model = try QwenVisionEncoder(configuration: config)
            self.visionModel = try VNCoreMLModel(for: model.model)
        } catch {
            print("Failed to load CoreML model: \(error)")
        }
        */
        // Placeholder for now as we don't have the .mlpackage in this environment
    }
    
    // MARK: - Pipeline Execution
    
    public func processFrame(texture: MTLTexture) {
        // 1. Resize / Pre-process if needed
        // For Qwen-VL, we often want the native patches, but for "The Glance", we might downscale.
        
        // Ensure we have a coarse texture target
        if coarseTexture == nil {
            let desc = MTLTextureDescriptor.texture2DDescriptor(pixelFormat: .rgba16Float, width: 448, height: 448, mipmapped: false)
            desc.usage = [.shaderWrite, .shaderRead]
            coarseTexture = device.makeTexture(descriptor: desc)
        }
        
        // 2. Run Metal Pre-processing
        // NOTE: If using CoreML with Vision, Vision handles normalization usually.
        // BUT if we want "Bit-Exact" control or specific pre-processing not supported by Vision layers, we do it here.
        // For this plan, we rely on CoreML doing the heavy lifting for the Encoder, 
        // but we might use our 'smartCropper' here later.
        
        // 3. Execute Vision Request
        guard let model = visionModel else { return }
        
        let request = VNCoreMLRequest(model: model) { [weak self] request, error in
            guard let self = self else { return }
            if let results = request.results as? [VNCoreMLFeatureValueObservation],
               let feature = results.first?.featureValue.multiArrayValue {
                // 4. Success - Embeddings ready
                self.onEmbeddingsGenerated?(feature)
            }
        }
        
        // Run on the ANE
        request.imageCropAndScaleOption = .scaleFill
        
        let handler = VNImageRequestHandler(cvPixelBuffer: texture.toCVPixelBuffer()!, orientation: .up, options: [:])
        
        do {
            try handler.perform([request])
        } catch {
            print("Vision failed: \(error)")
        }
    }
}

// MARK: - Helper (Texture -> CVPixelBuffer)
// Needed because Vision.framework prefers CVPixelBuffers over raw MTLTextures usually,
// although it can handle CIImage(mtlTexture:).

extension MTLTexture {
    func toCVPixelBuffer() -> CVPixelBuffer? {
        // In a real optimized app, this would use a CVPixelBuffer created via CVMetalTextureCache
        // to avoid a copy. The backing IOSurface is shared.
        // This is just a stub to indicate the logic flow.
        return nil 
    }
}
